---
title: "TDDE01 Lab 2"
---

Setup
```{r}
setwd("~/dev/TDDE01/lab2")
# install.packages("glmnet")
# install.packages("ggplot2")
# install.packages("reshape2")
# install.packages("tidyr")
# install.packages("tree")
# install.packages("latex2exp")
```

# Assignment 1
Divide the data randomly in train and test.
```{r}
data = read.csv("tecator.csv", header=T)[,2:102]

N = nrow(data)
set.seed(12345)
ids = sample(1:N, floor(N*0.5))
data.train = data[ids,] 
data.test = data[-ids,]
```

## Task 1
```{r}
model.linear = lm(Fat~., data=data.train)
pred.train = predict(model.linear, data.train, type = "response")
pred.test = predict(model.linear, data.test, type = "response")
mse.train = mean((pred.train-data.train$Fat)^2)
mse.test = mean((pred.test-data.test$Fat)^2)
print(sprintf("MSE_train: %f, MSE_test: %f", mse.train, mse.test))
```

## Task 3
```{r}
library(glmnet)
library(ggplot2)
library(reshape2)
library(latex2exp)

# Plots the values of the channels as a function of log theta
plt.reg = function(glmmodel, title) {
  res = as.data.frame(as.matrix(glmmodel$beta))
  res$channel = rownames(res)
  res = melt(res, id = "channel")
  res$variable = as.numeric(gsub("s", "", res$variable))
  res$lambda = glmmodel$lambda[res$variable+1]
  
  ggplot(res, aes(lambda, value, color = channel)) +
    geom_line() +
    scale_x_continuous(trans='log') +
    xlab(TeX("\\log \\lambda")) +
    ylab(TeX("Values of the $\\textbf{\\theta}_i$")) +
    ggtitle(title) +
    theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
}

model.lasso = glmnet(data.train[,1:100], data.train$Fat, alpha=1)
plt.reg(model.lasso, TeX("Lasso regression model parameters as a function of \\lambda"))

lasso.res = as.matrix(model.lasso$beta)
colnames(lasso.res) = model.lasso$lambda
lasso.nzero = apply(lasso.res, FUN = function(M) { 100 - length(as.vector(M[M == 0])) } , MARGIN = 2)
lasso.3 = lasso.res[, which(lasso.nzero == 3)]
print(colnames(lasso.3))
```

## Task 4
```{r}
model.ridge = glmnet(data.train[,1:100], data.train$Fat, alpha = 0)
plt.reg(model.ridge, TeX("Ridge regression model parameters as a function of \\lambda"))
```

## Task 5
TODO: What is the CV score? Using mean mses for now
```{r}
model.cv = cv.glmnet(as.matrix(data.train[,1:100]), as.matrix(data.train$Fat), alpha = 1)
res = data.frame(lambda=model.cv$lambda, cvm=model.cv$cvm)

ggplot(res, aes(lambda, cvm)) +
  geom_line() +
  ylab(TeX("Mean $MSE$")) + 
  xlab(TeX("\\log \\lambda")) +
  scale_x_continuous(trans="log") +
  ggtitle(TeX("Mean $MSE$ as a function of \\lambda")) +
  theme(plot.title = element_text(hjust = 0.5))

lambda.opt = model.cv$lambda.min
lambda.opt.idx = which(model.cv$lambda == lambda.opt)
lambda.opt.nzero = model.cv$nzero[lambda.opt.idx]
print(sprintf("Optimal lambda: %f, features > 0: %d", lambda.opt, lambda.opt.nzero))

opt.predict = predict(model.cv$glmnet.fit, as.matrix(data.test[,1:100]), lambda.opt, type = "response")

res = data.frame(x = rownames(data.test), true=data.test$Fat, pred=opt.predict[,1])
res = melt(res, id=c("x"))
ggplot(res, aes(x, value, color=variable)) +
  geom_point() +
  xlab("Test instances") +
  ylab("y") +
  theme(axis.text.x =element_blank())
```

# Assignment 2
## Task 1
```{r}
data = read.csv("bank-full.csv", sep = ";", stringsAsFactors = T, header = T)
data = within(data, y <- relevel(y, ref="yes")) # Make yes the default level
data$duration = c()
N = nrow(data)
set.seed(12345)
ids = sample(1:N, floor(N*0.4))
data.train = data[ids,]

ids1 = setdiff(1:N, ids)
set.seed(12345)
ids2 = sample(ids1, floor(N*0.3))
data.val = data[ids2,]

ids3 = setdiff(ids1, ids2)
data.test = data[ids3,]
```

## Task 2
```{r}
library(tree)

# Returns the missclassification rate for a given tree and data
get_errors = function(tree, newdata) {
  pred = predict(tree, newdata, type = "class")
  cm = table(newdata$y, pred)
  1 - sum(diag(cm)) / nrow(newdata)
}

tree.a = tree(y ~ ., data = data.train)
err.a.train = get_errors(tree.a, data.train)
err.a.val = get_errors(tree.a, data.val)
size.a = summary(tree.a)$size

tree.b = tree(y ~ ., data = data.train, minsize = 7000)
err.b.train = get_errors(tree.b, data.train)
err.b.val = get_errors(tree.b, data.val)
size.b = summary(tree.b)$size

tree.c = tree(y ~ ., data.train, mindev = 0.0005)
err.c.train = get_errors(tree.c, data.train)
err.c.val = get_errors(tree.c, data.val)
size.c = summary(tree.c)$size

comparison = data.frame(
  c(err.a.train, err.b.train, err.c.train),
  c(err.a.val, err.b.val, err.c.val),
  c(size.a, size.b, size.c)
)
dimnames(comparison) = list(c("A", "B", "C"), c("Err train", "Err val", "Size"))
```

## Task 3
```{r}
score.train = rep(0, 49)
score.val = rep(0, 49)
for (c in 2:50) {
  tree.pruned = prune.tree(tree.c, best = c)
  pred.train = predict(tree.pruned, data.train, type = "tree")
  pred.val = predict(tree.pruned, data.val, type = "tree")
  score.train[c - 1] = deviance(pred.train)
  score.val[c - 1] = deviance(pred.val)
}

deviances = data.frame(x = 2:50,
                       train = score.train,
                       validation = score.val)
deviances = melt(deviances, id = c("x"))

ggplot(deviances) +
  geom_line(aes(x = x, y = value, color = variable)) +
  xlab("Leaf count") +
  ylab("Deviance")

leaf.opt = which(score.val == min(score.val))
```

## Task 4
```{r}
TPR = function(cm) { cm[1,1] / (cm[1,1] + cm[1,2]) }
PPV = function(cm) { cm[1,1] / (cm[1,1] + cm[2,1]) }

tree.opt = prune.tree(tree.c, best = leaf.opt)
pred.test = predict(tree.opt, data.test, type = "class")
cm.test = table(data.test$y, pred.test)
acc.test = sum(diag(cm.test)) / nrow(data.test)

tpr.test = TPR(cm.test)
ppv.test = PPV(cm.test)
f1.test = 2 * (ppv.test * tpr.test) / (ppv.test + tpr.test)
print(sprintf("Test acc: %f, Test f1: %f", acc.test, f1.test))
```

## Task 5
TODO: How matrix, results unexpected.
```{r}
library(rpart)
tree.lossmatrix = rpart(y ~ ., data = data.train, method = "class", parms = list(loss = matrix(c(0, 1, 5, 0), ncol=2)))
pred.lossmatrix = predict(tree.lossmatrix, data.test, type = "class")
cm.lossmatrix = table(data.test$y, pred.lossmatrix)
print(cm.lossmatrix)
```

## Task 6
TODO: ROC be fucked.
```{r}
FPR = function(cm) { cm[1,2] / (cm[1,2] + cm[2,2]) }

# Optimal tree prediction
pred.tree = predict(tree.opt, data.test, type = "vector")

# Logistic model and prediction
lg = glm(y~., data = data.train, family = "binomial")
pred.lg = predict(lg, data.test, type = "response")

tpr.tree = c()
fpr.tree = c()
tpr.lg = c()
fpr.lg = c()
for (pi in seq(from=0.05, to=0.95, by=0.05)) {
  # Tree, first element of row is the yes probability
  pred.tree.pi = apply(pred.tree, 1, FUN = function(row) { if (row[1] > pi) "yes" else "no" })
  cm.tree = table(data.test$y, factor(pred.tree.pi, levels=c("yes", "no")))
  tpr.tree = c(tpr.tree, TPR(cm.tree))
  fpr.tree = c(fpr.tree, FPR(cm.tree))
  print(cm.tree)
  
  # Logistic regression,
  pred.lg.pi = sapply(pred.lg, FUN = function(y_hat) { if(y_hat > pi) "yes" else "no" })
  cm.lg = table(data.test$y, factor(pred.lg.pi, levels=c("yes", "no")))
  tpr.lg = c(tpr.lg, TPR(cm.lg))
  fpr.lg = c(fpr.lg, FPR(cm.lg))
}

res = data.frame(fpr.tree, tpr.tree, fpr.lg, tpr.lg)

ggplot(res) +
  geom_line(aes(x=fpr.lg, y=tpr.lg), color="red") +
  geom_line(aes(x=fpr.tree, y=tpr.tree), color="blue") +
  ylab("TPR") +
  xlab("FPR")
```

# Assignment 3
## Task 1
```{r}
library(caret)
data = read.csv("communities.csv", header = T)
scaler = preProcess(data[-ncol(data)])
data = predict(scaler, data)

N = nrow(data)
X = as.matrix(data)
S = (t(X) %*% X) / N
eigenvectors = eigen(S)
```

## Task 2
```{r}
res = princomp(X)
summary(res)
```

