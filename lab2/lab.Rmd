---
title: "TDDE01 Lab 2"
---

Setup
```{r}
setwd("~/dev/TDDE01/lab2")
# install.packages("glmnet")
# install.packages("ggplot2")
# install.packages("reshape2")
# install.packages("tidyr")
# install.packages("tree")
# install.packages("latex2exp")
# install.packages("pROC")
```

# Assignment 1
Divide the data randomly in train and test.
```{r}
data = read.csv("tecator.csv", header=T)[,2:102]

N = nrow(data)
set.seed(12345)
ids = sample(1:N, floor(N*0.5))
data.train = data[ids,] 
data.test = data[-ids,]
```

## Task 1
```{r}
model.linear = lm(Fat~., data=data.train)
pred.train = predict(model.linear, data.train, type = "response")
pred.test = predict(model.linear, data.test, type = "response")
mse.train = mean((pred.train-data.train$Fat)^2)
mse.test = mean((pred.test-data.test$Fat)^2)
print(sprintf("MSE_train: %f, MSE_test: %f", mse.train, mse.test))
```

## Task 3
```{r}
library(glmnet)
library(ggplot2)
library(reshape2)
library(latex2exp)

# Plots the values of the channels as a function of log theta
plt.reg = function(glmmodel, title) {
  res = as.data.frame(as.matrix(glmmodel$beta))
  res$channel = rownames(res)
  res = melt(res, id = "channel")
  res$variable = as.numeric(gsub("s", "", res$variable))
  res$lambda = glmmodel$lambda[res$variable+1]
  
  ggplot(res, aes(lambda, value, color = channel)) +
    geom_line() +
    scale_x_continuous(trans='log') +
    xlab(TeX("\\log \\lambda")) +
    ylab(TeX("Values of the $\\textbf{\\theta}_i$")) +
    ggtitle(title) +
    theme(legend.position = "none", plot.title = element_text(hjust = 0.5))
}

model.lasso = glmnet(data.train[,1:100], data.train$Fat, alpha=1)
plt.reg(model.lasso, TeX("Lasso regression model parameters as a function of \\lambda"))

lasso.res = as.matrix(model.lasso$beta)
colnames(lasso.res) = model.lasso$lambda
lasso.nzero = apply(lasso.res, FUN = function(M) { 100 - length(as.vector(M[M == 0])) }, MARGIN = 2)
lasso.3 = lasso.res[, which(lasso.nzero == 3)]
print(colnames(lasso.3))
```

## Task 4
```{r}
model.ridge = glmnet(data.train[,1:100], data.train$Fat, alpha = 0)
plt.reg(model.ridge, TeX("Ridge regression model parameters as a function of \\lambda"))
```

## Task 5
TODO: What is the CV score? Using mean mses for now
```{r}
model.cv = cv.glmnet(as.matrix(data.train[,1:100]), as.matrix(data.train$Fat), alpha = 1)
res = data.frame(lambda=model.cv$lambda, cvm=model.cv$cvm)

ggplot(res, aes(lambda, cvm)) +
  geom_line() +
  ylab(TeX("Mean $MSE$")) + 
  xlab(TeX("\\log \\lambda")) +
  scale_x_continuous(trans="log") +
  ggtitle(TeX("Mean $MSE$ as a function of \\lambda")) +
  theme(plot.title = element_text(hjust = 0.5))

lambda.opt = model.cv$lambda.min
lambda.opt.idx = which(model.cv$lambda == lambda.opt)
lambda.opt.nzero = model.cv$nzero[lambda.opt.idx]
print(sprintf("Optimal lambda: %f, features > 0: %d", lambda.opt, lambda.opt.nzero))

opt.predict = predict(model.cv$glmnet.fit, as.matrix(data.test[,1:100]), lambda.opt, type = "response")

res = data.frame(x = rownames(data.test), true=data.test$Fat, pred=opt.predict[,1])
res = melt(res, id=c("x"))
ggplot(res, aes(x, value, color=variable)) +
  geom_point() +
  xlab("Test instances") +
  ylab("y") +
  theme(axis.text.x =element_blank())
```

# Assignment 2
## Task 1
```{r}
data = read.csv("bank-full.csv", sep = ";", stringsAsFactors = T, header = T)
data = within(data, y <- relevel(y, ref="yes")) # Make yes the reference level
data$duration = c()

N = nrow(data)
set.seed(12345)
ids = sample(1:N, floor(N*0.4))
data.train = data[ids,]

ids1 = setdiff(1:N, ids)
set.seed(12345)
ids2 = sample(ids1, floor(N*0.3))
data.val = data[ids2,]

ids3 = setdiff(ids1, ids2)
data.test = data[ids3,]
```

## Task 2
```{r}
library(tree)

# Returns the missclassification rate for a given tree and data
get_errors = function(tree, newdata) {
  pred = predict(tree, newdata, type = "class")
  cm = table(newdata$y, pred)
  1 - sum(diag(cm)) / nrow(newdata)
}

tree.a = tree(y ~ ., data = data.train)
err.a.train = get_errors(tree.a, data.train)
err.a.val = get_errors(tree.a, data.val)
size.a = summary(tree.a)$size

tree.b = tree(y ~ ., data = data.train, minsize = 7000)
err.b.train = get_errors(tree.b, data.train)
err.b.val = get_errors(tree.b, data.val)
size.b = summary(tree.b)$size

tree.c = tree(y ~ ., data.train, mindev = 0.0005)
err.c.train = get_errors(tree.c, data.train)
err.c.val = get_errors(tree.c, data.val)
size.c = summary(tree.c)$size

comparison = data.frame(
  c(err.a.train, err.b.train, err.c.train),
  c(err.a.val, err.b.val, err.c.val),
  c(size.a, size.b, size.c)
)
dimnames(comparison) = list(c("A", "B", "C"), c("Err train", "Err val", "Size"))
```

## Task 3
```{r}
score.train = rep(0, 49)
score.val = rep(0, 49)
for (c in 2:50) {
  tree.pruned = prune.tree(tree.c, best = c)
  pred.train = predict(tree.pruned, data.train, type = "tree")
  pred.val = predict(tree.pruned, data.val, type = "tree")
  score.train[c - 1] = deviance(pred.train)
  score.val[c - 1] = deviance(pred.val)
}

deviances = data.frame(x = 2:50,
                       train = score.train,
                       validation = score.val)
deviances = melt(deviances, id = c("x"))

ggplot(deviances) +
  geom_line(aes(x = x, y = value, color = variable)) +
  xlab("Leaf count") +
  ylab("Deviance")

leaf.opt = which(score.val == min(score.val))
print(sprintf("The optimal leaf count is %d", leaf.opt))

tree.opt = prune.tree(tree.c, best = leaf.opt)
print(summary(tree.opt))
plot(tree.opt)
text(tree.opt)
```

## Task 4
```{r}
TPR = function(cm) { cm[1,1] / (cm[1,1] + cm[1,2]) }
PPV = function(cm) { cm[1,1] / (cm[1,1] + cm[2,1]) }

pred.test = predict(tree.opt, data.test, type = "class")
cm.test = table(data.test$y, pred.test)
acc.test = sum(diag(cm.test)) / nrow(data.test)

tpr.test = TPR(cm.test)
ppv.test = PPV(cm.test)
f1.test = 2 * (ppv.test * tpr.test) / (ppv.test + tpr.test)
print(sprintf("Test acc: %f, Test F1-score: %f, Test recall rate: %f", acc.test, f1.test, tpr.test))
print(cm.test)
```

## Task 5
```{r}
pred.probs = predict(tree.opt, data.test, type = "vector")
pred.lossmatrix = ifelse(pred.probs[,1]/pred.probs[,2] > 1/5, "yes", "no")
cm.lossmatrix = table(data.test$y, pred.lossmatrix)
acc.lossmatrix = sum(diag(cm.lossmatrix)) / nrow(data.test)

tpr.lossmatrix = TPR(cm.lossmatrix)
ppv.lossmatrix = PPV(cm.lossmatrix)
f1.lossmatrix = 2 * (ppv.lossmatrix * tpr.lossmatrix) / (ppv.lossmatrix + tpr.lossmatrix)
print(sprintf("Lossmatrix test acc: %f, Lossmatrix test F1-score: %f, Recall: %f", acc.lossmatrix, f1.lossmatrix, tpr.lossmatrix))
print(cm.lossmatrix)
```

## Task 6
```{r}
FPR = function(cm) { cm[1,2] / (cm[1,2] + cm[2,2]) }

# Optimal tree prediction
pred.tree = predict(tree.opt, data.test, type = "vector")

# Logistic model and prediction
lg = glm(y~., data = data.train, family = "binomial")
pred.lg = predict(lg, data.test, type = "response")

tpr.tree = c()
fpr.tree = c()
tpr.lg = c()
fpr.lg = c()
for (pi in seq(from=0.05, to=0.95, by=0.05)) {
  # Tree, first element of row is the yes probability
  pred.tree.pi = ifelse(pred.tree[,1]>pi, "yes", "no")
  cm.tree = table(data.test$y, factor(pred.tree.pi, levels=c("yes", "no")))
  tpr.tree = c(tpr.tree, TPR(cm.tree))
  fpr.tree = c(fpr.tree, FPR(cm.tree))
  
  # Logistic regression
  pred.lg.pi = ifelse(pred.lg>pi, "yes", "no")
  cm.lg = table(data.test$y, factor(pred.lg.pi, levels=c("yes", "no")))
  print(cm.lg)
  tpr.lg = c(tpr.lg, TPR(cm.lg))
  fpr.lg = c(fpr.lg, FPR(cm.lg))
}

res = data.frame(fpr.tree, tpr.tree, fpr.lg, tpr.lg)

ggplot(res) +
  geom_line(aes(x=fpr.tree, y=tpr.tree), color="blue") +
  geom_line(aes(x=fpr.lg, y=tpr.lg), color="red") +
  ylab("TPR") +
  xlab("FPR")
```

# Assignment 3
## Task 1
```{r}
library(caret)
data = read.csv("communities.csv", header = T)
scaler = preProcess(data[-ncol(data)])
data = predict(scaler, data)

N = nrow(data)
X = as.matrix(data)
S = (t(X) %*% X) / N
eigen = eigen(S)

evalues.cum = sum(eigen$values)
relativesum = 0
for (i in 1:length(eigen$values)) {
  # How much relative variance is captured by the component
  relative = eigen$values[i] / evalues.cum
  relativesum = relativesum + relative
  if (relativesum >= 0.95) {
    print(sprintf("%d components are needed to capture 95 percent of the variance", i))
    break
  }
}

print(sprintf("Component 1 captures %f percent of the variance", 100 * eigen$values[1] / evalues.cum))
print(sprintf("Component 2 captures %f percent of the variance", 100 * eigen$values[2] / evalues.cum))
```

## Task 2
```{r}
res = princomp(X)
summary(res)
```

## Task 3
TODO: Errors are too good to be true.
```{r}
data = read.csv("communities.csv", header = T)
scaler = preProcess(data)
data = predict(scaler, data)

N = nrow(data)
Y = ncol(data)
ids = sample(1:N, floor(N*0.5))
data.train = data[ids,]
data.test = data[-ids,]

model = lm(data.train[,Y]~., data.train)
pred.train = predict(model, data.train)
pred.test = predict(model, data.test)
err.train = mean((pred.train - data.train[,Y]) ^ 2)
err.test = mean((pred.test - data.test[,Y]) ^ 2)
print(sprintf("Train MSE: %f, Test MSE: %f", err.train, err.test))
```

## Task 4
```{r}
lossfun = function(theta) {
  pred = as.matrix(data.train[,-Y]) %*% theta
  mean((pred - data.train[,Y]) ^ 2)
}

theta = rep(0, Y-1)
err.train = c()
err.test = c()

IT = 3000
for (i in 1:IT) {
  print(sprintf("Starting iteration %d/%d", i, IT))
  res = optim(theta, lossfun, method = "BFGS", control = list(maxit = 1))
  theta = res$par
  err.train = c(err.train, lossfun(theta))
  pred.test = as.matrix(data.test[,-Y]) %*% theta
  err.test = c(err.test, mean((pred.test - data.test[,Y]) ^ 2))
}

res = data.frame(x = 1:IT, train = err.train, test = err.test)
res = res[500:nrow(res),]
res = melt(res, id=c("x"))

ggplot(res, aes(x = x, y = value, color = variable)) +
  geom_line()
```

